# Bank Customer Churn Prediction â€“ Kaggle Tabular Project

## One Sentence Summary
This repository contains an implementation of a machine learning pipeline to predict bank customer churn, based on the [Playground Series - Season 4, Episode 1](https://www.kaggle.com/competitions/playground-series-s4e1) Kaggle competition.

## Overview

### Task Definition
The goal of the project is to build a binary classification model to predict whether a customer will close their bank account (`Exited` = 1) based on demographic and financial account features.

### Approach
This project formulates the task as a supervised classification problem. Preprocessing steps included one-hot encoding of categorical features, feature scaling for numerical columns, and dropping identifier columns. A Random Forest Classifier was used as the baseline model due to its robustness and compatibility with tabular data.

### Performance Summary
The final Random Forest model achieved a validation accuracy of approximately **X%** and a validation F1-score of **Y**. A Kaggle submission file was generated from test predictions.

## Summary of Work Done

### Data

- Type: Tabular CSV data
- Input: Demographic and financial information for each customer
- Target: `Exited` (0 = retained, 1 = exited)
- Size:
  - Training set: ~8,000 rows
  - Test set: ~4,000 rows

### Preprocessing and Clean-Up

- Dropped unnecessary columns (`id`, `CustomerId`, `Surname`)
- Encoded categorical features (`Gender`, `Geography`) using one-hot encoding
- Applied standard scaling to numerical features
- Checked and documented class imbalance (~20% exited)

### Data Visualization

- Histograms comparing distributions of numerical features by `Exited` status
- Bar plots and cross-tabulations for categorical features
- Observations indicated strong separation for features like `Age`, `Balance`, and `IsActiveMember`

## Problem Formulation

- Input: Cleaned feature matrix including both numerical and encoded categorical variables
- Output: Binary class label (0 or 1 indicating churn)

## Models

- Model: Random Forest Classifier
- Reason: Performs well on structured/tabular data with minimal tuning
- Hyperparameters: Default parameters with `random_state=42`

## Training

- Platform: Local Jupyter Notebook (MacBook Pro with Python 3.12)
- Frameworks: scikit-learn, pandas, matplotlib
- Time: Less than 1 minute (no GPU needed)
- No overfitting observed; training stopped after initial evaluation

## Performance Comparison

- Metrics: Accuracy, F1-Score, and confusion matrix on validation set
- Final validation accuracy: **X%**
- Submission results generated using the trained model applied to the Kaggle test set

## Conclusions

- The model successfully identifies predictive features of churn
- Features such as `Age`, `Balance`, and `IsActiveMember` were most impactful
- Random Forest is an effective baseline for this task

## Future Work

- Experiment with XGBoost or LightGBM
- Perform feature selection or SHAP analysis for interpretability
- Address class imbalance using SMOTE or class weights
- Automate hyperparameter tuning with GridSearchCV

## Software Setup

- Python 3.12
- pandas
- scikit-learn
- matplotlib
- seaborn

## Data Access

- Dataset is publicly available from:
  [Kaggle Playground Series - Season 4, Episode 1](https://www.kaggle.com/competitions/playground-series-s4e1)

## Performance Evaluation

- Run `Kaggle_Tabular_Data.ipynb` to view training metrics and generate predictions
- Final results can be submitted to Kaggle for scoring

## Citations

- Kaggle Competition: https://www.kaggle.com/competitions/playground-series-s4e1
- scikit-learn documentation: https://scikit-learn.org/

