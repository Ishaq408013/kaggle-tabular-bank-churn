# Bank Customer Churn Prediction â€“ Kaggle Tabular Project

* **One Sentence Summary**  
  This repository contains an attempt to predict bank customer churn using structured data from the [Playground Series - Season 4, Episode 1](https://www.kaggle.com/competitions/playground-series-s4e1) Kaggle challenge.

## Overview

* **Definition of the task / challenge**  
  The task is to predict whether a customer will close their bank account (`Exited` = 1) based on a variety of demographic and financial features. The dataset is tabular, and this is a binary classification problem.

* **Approach**  
  This repository treats the problem as a supervised learning classification task. A Random Forest classifier is trained on preprocessed data. Preprocessing included one-hot encoding for categorical features, standardization of numeric features, and removal of ID fields.

* **Summary of the performance achieved**  
  The final model achieved a validation accuracy of approximately **X%** and an F1-score of **Y** on the held-out validation data. A Kaggle submission was generated for external evaluation.

## Summary of Workdone

### Data

* **Data**:
  * Type:
    * Input: Tabular data (CSV format) with demographic and financial features
    * Output: Binary flag (`Exited`: 1 = churned, 0 = retained)
  * Size:
    * Training set: ~8,000 rows
    * Test set: ~4,000 rows
  * Instances (Train, Test, Validation Split):
    * 64% training, 16% validation, 20% internal test

#### Preprocessing / Clean Up

* Dropped non-informative columns: `id`, `CustomerId`, and `Surname`
* One-hot encoded the `Gender` and `Geography` columns
* Standardized numerical features using `StandardScaler`
* Verified there were no missing values
* Analyzed class imbalance (roughly 20% of customers churned)
* Detected and described outliers using the IQR method

#### Data Visualization

* Created histograms of numerical features grouped by `Exited`
* Created bar plots and cross-tabulations for categorical features
* Found that `Age`, `Balance`, and `IsActiveMember` showed the strongest separation between classes

## Problem Formulation

* **Input**: Tabular features after encoding and scaling
* **Output**: Binary label indicating churn status (0 or 1)

## Models

* Random Forest Classifier from `scikit-learn`
* Reason for choice: Handles tabular data well, robust to outliers and requires minimal hyperparameter tuning
* Hyperparameters used: Default (`random_state=42`)

## Training

* Platform: Jupyter Notebook on local machine (MacBook Pro, Python 3.12)
* Libraries used: `pandas`, `scikit-learn`, `matplotlib`, `seaborn`
* Training time: Less than 1 minute (model is not compute intensive)
* Training was stopped after baseline model evaluation

## Performance Comparison

* Key metrics: Accuracy, F1-score, confusion matrix
* Final validation accuracy: **X%**
* Submission file (`submission.csv`) was generated from predictions on the test dataset

## Conclusions

* Random Forest provided reliable performance with default settings
* Features like `Age`, `Balance`, and `IsActiveMember` were most predictive of churn
* The pipeline provides a solid baseline and is easy to build upon

## Future Work

* Evaluate performance using boosting models like XGBoost or LightGBM
* Perform feature importance and SHAP value analysis
* Explore oversampling methods like SMOTE to balance the classes
* Conduct hyperparameter tuning using GridSearchCV


## Software Setup

- Python 3.12
- pandas
- scikit-learn
- matplotlib
- seaborn

## Data Access

- Dataset is publicly available from:
  [Kaggle Playground Series - Season 4, Episode 1](https://www.kaggle.com/competitions/playground-series-s4e1)

## Performance Evaluation

- Run `Kaggle_Tabular_Data.ipynb` to view training metrics and generate predictions
- Final results can be submitted to Kaggle for scoring

## Citations

- Kaggle Competition: https://www.kaggle.com/competitions/playground-series-s4e1
- scikit-learn documentation: https://scikit-learn.org/

